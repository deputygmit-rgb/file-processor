{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 3.0,
  "eval_steps": 500,
  "global_step": 642,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.023446658851113716,
      "grad_norm": 0.20510348677635193,
      "learning_rate": 0.0004968847352024923,
      "loss": 5.0267,
      "step": 5
    },
    {
      "epoch": 0.04689331770222743,
      "grad_norm": 0.263969749212265,
      "learning_rate": 0.0004929906542056076,
      "loss": 4.901,
      "step": 10
    },
    {
      "epoch": 0.07033997655334115,
      "grad_norm": 0.27724865078926086,
      "learning_rate": 0.0004890965732087227,
      "loss": 4.7677,
      "step": 15
    },
    {
      "epoch": 0.09378663540445487,
      "grad_norm": 0.3714906573295593,
      "learning_rate": 0.00048520249221183803,
      "loss": 4.6737,
      "step": 20
    },
    {
      "epoch": 0.11723329425556858,
      "grad_norm": 0.3492735028266907,
      "learning_rate": 0.00048130841121495326,
      "loss": 4.5402,
      "step": 25
    },
    {
      "epoch": 0.1406799531066823,
      "grad_norm": 0.3520643711090088,
      "learning_rate": 0.00047741433021806855,
      "loss": 4.4005,
      "step": 30
    },
    {
      "epoch": 0.16412661195779601,
      "grad_norm": 0.38270291686058044,
      "learning_rate": 0.0004735202492211838,
      "loss": 4.334,
      "step": 35
    },
    {
      "epoch": 0.18757327080890973,
      "grad_norm": 0.3901587426662445,
      "learning_rate": 0.0004696261682242991,
      "loss": 4.218,
      "step": 40
    },
    {
      "epoch": 0.21101992966002345,
      "grad_norm": 0.40028274059295654,
      "learning_rate": 0.0004657320872274143,
      "loss": 4.2112,
      "step": 45
    },
    {
      "epoch": 0.23446658851113716,
      "grad_norm": 0.44358253479003906,
      "learning_rate": 0.0004618380062305296,
      "loss": 4.048,
      "step": 50
    },
    {
      "epoch": 0.25791324736225085,
      "grad_norm": 0.545936107635498,
      "learning_rate": 0.00045794392523364484,
      "loss": 3.8736,
      "step": 55
    },
    {
      "epoch": 0.2813599062133646,
      "grad_norm": 0.5305270552635193,
      "learning_rate": 0.00045404984423676013,
      "loss": 3.9281,
      "step": 60
    },
    {
      "epoch": 0.3048065650644783,
      "grad_norm": 0.4916532635688782,
      "learning_rate": 0.00045015576323987537,
      "loss": 3.8606,
      "step": 65
    },
    {
      "epoch": 0.32825322391559203,
      "grad_norm": 0.5734147429466248,
      "learning_rate": 0.00044626168224299066,
      "loss": 3.6987,
      "step": 70
    },
    {
      "epoch": 0.3516998827667057,
      "grad_norm": 0.5118005275726318,
      "learning_rate": 0.0004423676012461059,
      "loss": 3.802,
      "step": 75
    },
    {
      "epoch": 0.37514654161781946,
      "grad_norm": 0.568626344203949,
      "learning_rate": 0.00043847352024922124,
      "loss": 3.6554,
      "step": 80
    },
    {
      "epoch": 0.39859320046893315,
      "grad_norm": 0.7861941456794739,
      "learning_rate": 0.0004345794392523365,
      "loss": 3.4573,
      "step": 85
    },
    {
      "epoch": 0.4220398593200469,
      "grad_norm": 0.5431568026542664,
      "learning_rate": 0.00043068535825545177,
      "loss": 3.5958,
      "step": 90
    },
    {
      "epoch": 0.4454865181711606,
      "grad_norm": 0.6437291502952576,
      "learning_rate": 0.000426791277258567,
      "loss": 3.635,
      "step": 95
    },
    {
      "epoch": 0.46893317702227433,
      "grad_norm": 0.5751447677612305,
      "learning_rate": 0.0004228971962616823,
      "loss": 3.5358,
      "step": 100
    },
    {
      "epoch": 0.492379835873388,
      "grad_norm": 0.9143399000167847,
      "learning_rate": 0.00041900311526479753,
      "loss": 3.4399,
      "step": 105
    },
    {
      "epoch": 0.5158264947245017,
      "grad_norm": 0.5653455257415771,
      "learning_rate": 0.0004151090342679128,
      "loss": 3.6314,
      "step": 110
    },
    {
      "epoch": 0.5392731535756154,
      "grad_norm": 0.5606838464736938,
      "learning_rate": 0.00041121495327102806,
      "loss": 3.3962,
      "step": 115
    },
    {
      "epoch": 0.5627198124267292,
      "grad_norm": 0.6790542006492615,
      "learning_rate": 0.00040732087227414335,
      "loss": 3.5707,
      "step": 120
    },
    {
      "epoch": 0.5861664712778429,
      "grad_norm": 0.6155697107315063,
      "learning_rate": 0.0004034267912772586,
      "loss": 3.449,
      "step": 125
    },
    {
      "epoch": 0.6096131301289566,
      "grad_norm": 0.8957760334014893,
      "learning_rate": 0.0003995327102803739,
      "loss": 3.4351,
      "step": 130
    },
    {
      "epoch": 0.6330597889800703,
      "grad_norm": 0.8396928906440735,
      "learning_rate": 0.0003956386292834891,
      "loss": 3.3768,
      "step": 135
    },
    {
      "epoch": 0.6565064478311841,
      "grad_norm": 0.8362367749214172,
      "learning_rate": 0.0003917445482866044,
      "loss": 3.2747,
      "step": 140
    },
    {
      "epoch": 0.6799531066822978,
      "grad_norm": 0.6414721608161926,
      "learning_rate": 0.00038785046728971964,
      "loss": 3.38,
      "step": 145
    },
    {
      "epoch": 0.7033997655334114,
      "grad_norm": 0.591597318649292,
      "learning_rate": 0.0003839563862928349,
      "loss": 3.3337,
      "step": 150
    },
    {
      "epoch": 0.7268464243845252,
      "grad_norm": 0.6263188719749451,
      "learning_rate": 0.00038006230529595016,
      "loss": 3.3622,
      "step": 155
    },
    {
      "epoch": 0.7502930832356389,
      "grad_norm": 0.6829550266265869,
      "learning_rate": 0.00037616822429906545,
      "loss": 3.4435,
      "step": 160
    },
    {
      "epoch": 0.7737397420867527,
      "grad_norm": 1.0519641637802124,
      "learning_rate": 0.0003722741433021807,
      "loss": 3.2704,
      "step": 165
    },
    {
      "epoch": 0.7971864009378663,
      "grad_norm": 0.713695764541626,
      "learning_rate": 0.000368380062305296,
      "loss": 3.2335,
      "step": 170
    },
    {
      "epoch": 0.82063305978898,
      "grad_norm": 0.8367999792098999,
      "learning_rate": 0.0003644859813084112,
      "loss": 3.3446,
      "step": 175
    },
    {
      "epoch": 0.8440797186400938,
      "grad_norm": 1.0770478248596191,
      "learning_rate": 0.0003605919003115265,
      "loss": 3.2305,
      "step": 180
    },
    {
      "epoch": 0.8675263774912075,
      "grad_norm": 0.7427259087562561,
      "learning_rate": 0.00035669781931464174,
      "loss": 3.2799,
      "step": 185
    },
    {
      "epoch": 0.8909730363423212,
      "grad_norm": 0.7496123313903809,
      "learning_rate": 0.00035280373831775703,
      "loss": 3.3401,
      "step": 190
    },
    {
      "epoch": 0.9144196951934349,
      "grad_norm": 0.6947504878044128,
      "learning_rate": 0.00034890965732087227,
      "loss": 3.2498,
      "step": 195
    },
    {
      "epoch": 0.9378663540445487,
      "grad_norm": 0.6768112778663635,
      "learning_rate": 0.00034501557632398756,
      "loss": 3.1333,
      "step": 200
    },
    {
      "epoch": 0.9613130128956624,
      "grad_norm": 0.7004496455192566,
      "learning_rate": 0.0003411214953271028,
      "loss": 3.1503,
      "step": 205
    },
    {
      "epoch": 0.984759671746776,
      "grad_norm": 0.7544453144073486,
      "learning_rate": 0.0003372274143302181,
      "loss": 3.0684,
      "step": 210
    },
    {
      "epoch": 1.0046893317702228,
      "grad_norm": 0.7011123299598694,
      "learning_rate": 0.0003333333333333333,
      "loss": 3.1367,
      "step": 215
    },
    {
      "epoch": 1.0281359906213365,
      "grad_norm": 0.6391891837120056,
      "learning_rate": 0.0003294392523364486,
      "loss": 3.2519,
      "step": 220
    },
    {
      "epoch": 1.0515826494724503,
      "grad_norm": 0.6603257656097412,
      "learning_rate": 0.00032554517133956385,
      "loss": 3.0043,
      "step": 225
    },
    {
      "epoch": 1.0750293083235638,
      "grad_norm": 0.60146164894104,
      "learning_rate": 0.00032165109034267914,
      "loss": 3.1951,
      "step": 230
    },
    {
      "epoch": 1.0984759671746775,
      "grad_norm": 0.9346717596054077,
      "learning_rate": 0.0003177570093457944,
      "loss": 3.2285,
      "step": 235
    },
    {
      "epoch": 1.1219226260257913,
      "grad_norm": 0.6914317011833191,
      "learning_rate": 0.00031386292834890967,
      "loss": 3.3009,
      "step": 240
    },
    {
      "epoch": 1.145369284876905,
      "grad_norm": 0.6248909831047058,
      "learning_rate": 0.0003099688473520249,
      "loss": 3.1618,
      "step": 245
    },
    {
      "epoch": 1.1688159437280188,
      "grad_norm": 0.6371327042579651,
      "learning_rate": 0.0003060747663551402,
      "loss": 3.0351,
      "step": 250
    },
    {
      "epoch": 1.1922626025791325,
      "grad_norm": 0.7427291870117188,
      "learning_rate": 0.00030218068535825543,
      "loss": 3.1946,
      "step": 255
    },
    {
      "epoch": 1.2157092614302463,
      "grad_norm": 0.7177122831344604,
      "learning_rate": 0.0002982866043613707,
      "loss": 3.1706,
      "step": 260
    },
    {
      "epoch": 1.2391559202813598,
      "grad_norm": 0.7275316715240479,
      "learning_rate": 0.00029439252336448596,
      "loss": 3.0486,
      "step": 265
    },
    {
      "epoch": 1.2626025791324738,
      "grad_norm": 0.6693520545959473,
      "learning_rate": 0.00029049844236760125,
      "loss": 3.1601,
      "step": 270
    },
    {
      "epoch": 1.2860492379835873,
      "grad_norm": 0.6535589694976807,
      "learning_rate": 0.0002866043613707165,
      "loss": 3.1042,
      "step": 275
    },
    {
      "epoch": 1.309495896834701,
      "grad_norm": 0.7916558980941772,
      "learning_rate": 0.00028271028037383177,
      "loss": 3.1257,
      "step": 280
    },
    {
      "epoch": 1.3329425556858148,
      "grad_norm": 0.7617619633674622,
      "learning_rate": 0.000278816199376947,
      "loss": 3.1077,
      "step": 285
    },
    {
      "epoch": 1.3563892145369285,
      "grad_norm": 0.7999894618988037,
      "learning_rate": 0.0002749221183800623,
      "loss": 3.0902,
      "step": 290
    },
    {
      "epoch": 1.3798358733880423,
      "grad_norm": 0.8521850109100342,
      "learning_rate": 0.00027102803738317753,
      "loss": 3.1014,
      "step": 295
    },
    {
      "epoch": 1.403282532239156,
      "grad_norm": 0.7816413044929504,
      "learning_rate": 0.0002671339563862928,
      "loss": 2.9249,
      "step": 300
    },
    {
      "epoch": 1.4267291910902697,
      "grad_norm": 0.796165406703949,
      "learning_rate": 0.00026323987538940806,
      "loss": 3.1488,
      "step": 305
    },
    {
      "epoch": 1.4501758499413833,
      "grad_norm": 0.6896616816520691,
      "learning_rate": 0.00025934579439252335,
      "loss": 3.0994,
      "step": 310
    },
    {
      "epoch": 1.473622508792497,
      "grad_norm": 0.6891331076622009,
      "learning_rate": 0.0002554517133956386,
      "loss": 3.0431,
      "step": 315
    },
    {
      "epoch": 1.4970691676436108,
      "grad_norm": 0.7243983745574951,
      "learning_rate": 0.0002515576323987539,
      "loss": 3.1876,
      "step": 320
    },
    {
      "epoch": 1.5205158264947245,
      "grad_norm": 0.6908588409423828,
      "learning_rate": 0.00024766355140186917,
      "loss": 3.1665,
      "step": 325
    },
    {
      "epoch": 1.5439624853458382,
      "grad_norm": 0.7002302408218384,
      "learning_rate": 0.00024376947040498443,
      "loss": 3.0828,
      "step": 330
    },
    {
      "epoch": 1.567409144196952,
      "grad_norm": 0.7306332588195801,
      "learning_rate": 0.0002398753894080997,
      "loss": 3.1658,
      "step": 335
    },
    {
      "epoch": 1.5908558030480657,
      "grad_norm": 0.6895581483840942,
      "learning_rate": 0.00023598130841121496,
      "loss": 3.1261,
      "step": 340
    },
    {
      "epoch": 1.6143024618991793,
      "grad_norm": 0.8005183339118958,
      "learning_rate": 0.00023208722741433022,
      "loss": 3.0825,
      "step": 345
    },
    {
      "epoch": 1.6377491207502932,
      "grad_norm": 0.6868237257003784,
      "learning_rate": 0.00022819314641744548,
      "loss": 3.0343,
      "step": 350
    },
    {
      "epoch": 1.6611957796014067,
      "grad_norm": 0.7002869248390198,
      "learning_rate": 0.00022429906542056075,
      "loss": 3.0324,
      "step": 355
    },
    {
      "epoch": 1.6846424384525205,
      "grad_norm": 0.6602380871772766,
      "learning_rate": 0.000220404984423676,
      "loss": 3.1553,
      "step": 360
    },
    {
      "epoch": 1.7080890973036342,
      "grad_norm": 0.6886919140815735,
      "learning_rate": 0.00021651090342679127,
      "loss": 3.0693,
      "step": 365
    },
    {
      "epoch": 1.731535756154748,
      "grad_norm": 0.6907283067703247,
      "learning_rate": 0.00021261682242990654,
      "loss": 3.0184,
      "step": 370
    },
    {
      "epoch": 1.7549824150058617,
      "grad_norm": 0.7802612781524658,
      "learning_rate": 0.0002087227414330218,
      "loss": 3.077,
      "step": 375
    },
    {
      "epoch": 1.7784290738569752,
      "grad_norm": 0.7299067974090576,
      "learning_rate": 0.00020482866043613706,
      "loss": 3.0439,
      "step": 380
    },
    {
      "epoch": 1.8018757327080892,
      "grad_norm": 0.738514244556427,
      "learning_rate": 0.00020093457943925233,
      "loss": 2.9757,
      "step": 385
    },
    {
      "epoch": 1.8253223915592027,
      "grad_norm": 0.6866450309753418,
      "learning_rate": 0.0001970404984423676,
      "loss": 3.0551,
      "step": 390
    },
    {
      "epoch": 1.8487690504103167,
      "grad_norm": 0.9351472854614258,
      "learning_rate": 0.00019314641744548285,
      "loss": 3.0869,
      "step": 395
    },
    {
      "epoch": 1.8722157092614302,
      "grad_norm": 1.0158599615097046,
      "learning_rate": 0.00018925233644859812,
      "loss": 3.0569,
      "step": 400
    },
    {
      "epoch": 1.895662368112544,
      "grad_norm": 0.7188437581062317,
      "learning_rate": 0.00018535825545171338,
      "loss": 2.9654,
      "step": 405
    },
    {
      "epoch": 1.9191090269636577,
      "grad_norm": 0.6977531909942627,
      "learning_rate": 0.00018146417445482864,
      "loss": 3.0116,
      "step": 410
    },
    {
      "epoch": 1.9425556858147714,
      "grad_norm": 0.7050319910049438,
      "learning_rate": 0.0001775700934579439,
      "loss": 3.0008,
      "step": 415
    },
    {
      "epoch": 1.9660023446658852,
      "grad_norm": 0.9383401870727539,
      "learning_rate": 0.0001736760124610592,
      "loss": 2.8845,
      "step": 420
    },
    {
      "epoch": 1.9894490035169987,
      "grad_norm": 0.9663084149360657,
      "learning_rate": 0.00016978193146417446,
      "loss": 2.9483,
      "step": 425
    },
    {
      "epoch": 2.0093786635404456,
      "grad_norm": 0.8156530857086182,
      "learning_rate": 0.00016588785046728972,
      "loss": 3.0203,
      "step": 430
    },
    {
      "epoch": 2.032825322391559,
      "grad_norm": 0.7710356712341309,
      "learning_rate": 0.000161993769470405,
      "loss": 3.0659,
      "step": 435
    },
    {
      "epoch": 2.056271981242673,
      "grad_norm": 0.6933649778366089,
      "learning_rate": 0.00015809968847352025,
      "loss": 2.9558,
      "step": 440
    },
    {
      "epoch": 2.0797186400937866,
      "grad_norm": 0.7348083853721619,
      "learning_rate": 0.00015420560747663551,
      "loss": 2.9582,
      "step": 445
    },
    {
      "epoch": 2.1031652989449006,
      "grad_norm": 0.7241766452789307,
      "learning_rate": 0.00015031152647975078,
      "loss": 2.9518,
      "step": 450
    },
    {
      "epoch": 2.126611957796014,
      "grad_norm": 0.7289255857467651,
      "learning_rate": 0.00014641744548286604,
      "loss": 3.1519,
      "step": 455
    },
    {
      "epoch": 2.1500586166471276,
      "grad_norm": 0.8359201550483704,
      "learning_rate": 0.0001425233644859813,
      "loss": 3.0946,
      "step": 460
    },
    {
      "epoch": 2.1735052754982416,
      "grad_norm": 0.7505865693092346,
      "learning_rate": 0.00013862928348909657,
      "loss": 3.0999,
      "step": 465
    },
    {
      "epoch": 2.196951934349355,
      "grad_norm": 0.6259127259254456,
      "learning_rate": 0.00013473520249221183,
      "loss": 3.0896,
      "step": 470
    },
    {
      "epoch": 2.220398593200469,
      "grad_norm": 0.8473994731903076,
      "learning_rate": 0.0001308411214953271,
      "loss": 2.8421,
      "step": 475
    },
    {
      "epoch": 2.2438452520515826,
      "grad_norm": 0.6740289330482483,
      "learning_rate": 0.00012694704049844236,
      "loss": 2.9274,
      "step": 480
    },
    {
      "epoch": 2.2672919109026966,
      "grad_norm": 0.713941752910614,
      "learning_rate": 0.00012305295950155765,
      "loss": 2.9893,
      "step": 485
    },
    {
      "epoch": 2.29073856975381,
      "grad_norm": 0.8071174621582031,
      "learning_rate": 0.0001191588785046729,
      "loss": 3.0976,
      "step": 490
    },
    {
      "epoch": 2.3141852286049236,
      "grad_norm": 0.7103583216667175,
      "learning_rate": 0.00011526479750778816,
      "loss": 2.9834,
      "step": 495
    },
    {
      "epoch": 2.3376318874560376,
      "grad_norm": 0.796281099319458,
      "learning_rate": 0.00011137071651090342,
      "loss": 2.9586,
      "step": 500
    },
    {
      "epoch": 2.361078546307151,
      "grad_norm": 0.7983179688453674,
      "learning_rate": 0.00010747663551401869,
      "loss": 2.9873,
      "step": 505
    },
    {
      "epoch": 2.384525205158265,
      "grad_norm": 0.7191662788391113,
      "learning_rate": 0.00010358255451713395,
      "loss": 2.9201,
      "step": 510
    },
    {
      "epoch": 2.4079718640093786,
      "grad_norm": 0.7762095332145691,
      "learning_rate": 9.968847352024921e-05,
      "loss": 2.9742,
      "step": 515
    },
    {
      "epoch": 2.4314185228604925,
      "grad_norm": 0.7266331911087036,
      "learning_rate": 9.579439252336449e-05,
      "loss": 3.0349,
      "step": 520
    },
    {
      "epoch": 2.454865181711606,
      "grad_norm": 0.7910456657409668,
      "learning_rate": 9.190031152647975e-05,
      "loss": 2.8542,
      "step": 525
    },
    {
      "epoch": 2.4783118405627196,
      "grad_norm": 0.6921886205673218,
      "learning_rate": 8.800623052959502e-05,
      "loss": 2.8917,
      "step": 530
    },
    {
      "epoch": 2.5017584994138335,
      "grad_norm": 0.7636828422546387,
      "learning_rate": 8.411214953271028e-05,
      "loss": 2.9845,
      "step": 535
    },
    {
      "epoch": 2.5252051582649475,
      "grad_norm": 0.8522152900695801,
      "learning_rate": 8.021806853582554e-05,
      "loss": 2.9757,
      "step": 540
    },
    {
      "epoch": 2.548651817116061,
      "grad_norm": 0.8884441256523132,
      "learning_rate": 7.632398753894081e-05,
      "loss": 3.0409,
      "step": 545
    },
    {
      "epoch": 2.5720984759671746,
      "grad_norm": 0.7734360098838806,
      "learning_rate": 7.242990654205607e-05,
      "loss": 2.851,
      "step": 550
    },
    {
      "epoch": 2.5955451348182885,
      "grad_norm": 0.7548694014549255,
      "learning_rate": 6.853582554517133e-05,
      "loss": 2.9666,
      "step": 555
    },
    {
      "epoch": 2.618991793669402,
      "grad_norm": 0.8018909096717834,
      "learning_rate": 6.46417445482866e-05,
      "loss": 2.9121,
      "step": 560
    },
    {
      "epoch": 2.6424384525205156,
      "grad_norm": 0.6488008499145508,
      "learning_rate": 6.074766355140187e-05,
      "loss": 2.8216,
      "step": 565
    },
    {
      "epoch": 2.6658851113716295,
      "grad_norm": 0.7435563206672668,
      "learning_rate": 5.685358255451714e-05,
      "loss": 2.9344,
      "step": 570
    },
    {
      "epoch": 2.6893317702227435,
      "grad_norm": 0.6671169400215149,
      "learning_rate": 5.29595015576324e-05,
      "loss": 2.8976,
      "step": 575
    },
    {
      "epoch": 2.712778429073857,
      "grad_norm": 0.7143837213516235,
      "learning_rate": 4.9065420560747664e-05,
      "loss": 2.9605,
      "step": 580
    },
    {
      "epoch": 2.7362250879249705,
      "grad_norm": 0.6809722781181335,
      "learning_rate": 4.517133956386293e-05,
      "loss": 2.9887,
      "step": 585
    },
    {
      "epoch": 2.7596717467760845,
      "grad_norm": 0.6765483617782593,
      "learning_rate": 4.127725856697819e-05,
      "loss": 2.9485,
      "step": 590
    },
    {
      "epoch": 2.783118405627198,
      "grad_norm": 0.7620468139648438,
      "learning_rate": 3.7383177570093454e-05,
      "loss": 2.9861,
      "step": 595
    },
    {
      "epoch": 2.806565064478312,
      "grad_norm": 0.7744371294975281,
      "learning_rate": 3.3489096573208724e-05,
      "loss": 2.9288,
      "step": 600
    },
    {
      "epoch": 2.8300117233294255,
      "grad_norm": 0.8593277931213379,
      "learning_rate": 2.9595015576323987e-05,
      "loss": 2.8946,
      "step": 605
    },
    {
      "epoch": 2.8534583821805395,
      "grad_norm": 0.8210504651069641,
      "learning_rate": 2.5700934579439254e-05,
      "loss": 3.0539,
      "step": 610
    },
    {
      "epoch": 2.876905041031653,
      "grad_norm": 0.6852453351020813,
      "learning_rate": 2.1806853582554517e-05,
      "loss": 3.0229,
      "step": 615
    },
    {
      "epoch": 2.9003516998827665,
      "grad_norm": 0.7347415089607239,
      "learning_rate": 1.791277258566978e-05,
      "loss": 2.8743,
      "step": 620
    },
    {
      "epoch": 2.9237983587338805,
      "grad_norm": 0.7312591075897217,
      "learning_rate": 1.4018691588785047e-05,
      "loss": 2.9987,
      "step": 625
    },
    {
      "epoch": 2.947245017584994,
      "grad_norm": 0.7372947335243225,
      "learning_rate": 1.0124610591900312e-05,
      "loss": 2.8296,
      "step": 630
    },
    {
      "epoch": 2.970691676436108,
      "grad_norm": 0.81463223695755,
      "learning_rate": 6.230529595015576e-06,
      "loss": 2.9697,
      "step": 635
    },
    {
      "epoch": 2.9941383352872215,
      "grad_norm": 0.7270558476448059,
      "learning_rate": 2.336448598130841e-06,
      "loss": 2.9161,
      "step": 640
    }
  ],
  "logging_steps": 5,
  "max_steps": 642,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 50,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 670780109094912.0,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
